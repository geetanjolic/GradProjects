
```{r}
#install.packages(c("plotly", "maps", "geofacet"))
library(dplyr)
library(tidyverse)
library(readxl)
library(writexl)
library(rio)
library(ggplot2)
library(cowplot)
library(randomForest)
library(rsample)
library(rpart)
library(rpart.plot)
library(broom)
library(yardstick)
library(caret)
library(cutpointr)
library(GGally)
library(RColorBrewer)
library(plotly)
library(maps)
library(geofacet)
library(ggpubr)

us_states <- map_data("state")

# Heat map homeless_individual_rate
df_pit_count_final %>% 
 # filter(year == 2019) %>%
  full_join(us_states, by = c("statename" = "region")) %>%
  ggplot(aes(long, lat)) + 
  geom_polygon(aes(group = group, fill = homeless_individual_rate)) + 
  coord_map() #+ facet_wrap(~year)


# Top 5 states each year with homeless_individual_rate
df_pit_count_final %>%
  group_by(year) %>%
  slice_max(homeless_individual_rate, n=5) %>%
  ggplot(aes(x=year, y= homeless_individual_rate, fill = state)) + 
  geom_col(position = "dodge", width = 0.7, color = "black") + 
  scale_fill_brewer(palette = "Pastel1")


# Difference plot between 2019 and 2015 for homeless_individual_rate
df_pit_count_final %>% 
  group_by(state) %>%
  filter(year == 2015 | year == 2019) %>%
  mutate(diff = homeless_individual_rate - homeless_individual_rate[1], 
         diff = replace(diff, row_number() == 1, NA),
         pos = diff>0) %>% 
  drop_na() %>%
  ggplot(aes(x = state, y = diff, fill = pos) ) + 
  geom_bar(stat="identity", colour = "black", size = 0.25) + 
  scale_fill_manual( values=c("gray", "red"), guide = 'none')  + 
  theme_bw() +
  labs(y = "Individual homeless rate", title = "Individual homeless rate change - 2015-2019") +
  theme(axis.text.x = element_text(angle = 90))


# Difference plot between 2019 and 2015 for homeless_family_rate
df_pit_count_final %>% 
  group_by(state) %>%
  filter(year == 2015 | year == 2016) %>%
  mutate(diff = homeless_family_rate - homeless_family_rate[1], 
         diff = replace(diff, row_number() == 1, NA)) %>% 
  drop_na() 


df_pit_count_1 <- df_pit_count_final %>%
  select(1,2,c(8:16)) %>%
  mutate(log_medium_income = log(median_income), log_fmr = log(fmr_studio), log_mgr = log(mgr_studio))

pi1 <-df_pit_count_1 %>%
  ggplot(aes(x=mgr_studio, y=homeless_individual_rate)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE,color="blue")
  
pi2 <- df_pit_count_1 %>%
  ggplot(aes(x=log_medium_income, y=homeless_individual_rate)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE,color="blue")

pi3 <- df_pit_count_1 %>%
  filter(homeless_family_rate < 0.5) %>%
  ggplot(aes(x=log_fmr, y=homeless_individual_rate)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE,color="blue")

pi4 <- df_pit_count_1 %>%
  ggplot(aes(x=min_wage, y=homeless_individual_rate)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE,color="blue")

pi5 <- df_pit_count_1 %>%
  ggplot(aes(x=unemployment_rate, y=homeless_individual_rate)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE,color="blue")

pi6 <- df_pit_count_final %>% 
  ggplot(aes(x=poverty_rate, y=homeless_individual_rate)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE,color="blue")

ggarrange(pi1,pi2,pi3,pi4,pi5,pi6,
          labels = c("A", "B", "C","D","E","F"),
          ncol = 3, nrow = 2)
```



Correlation of variables:
```{r}

cor(df_pit_count_final[,c(7,8,9,10,11,12,13,14)])

ggpairs(df_pit_count_final,columns = c(9, 11:16))
ggpairs(df_pit_count_final,columns = c(10:16))

cor(df_pit_count_final$homeless_individual_rate, log(df_pit_count_final$median_income))
```




Random Forest Models
```{r}
# Split the dataset into training and testing 
rfmodel1<- randomForest(homeless_individual_rate ~ fmr_studio, 
                         data = df_pit_count_final,
                         importance = TRUE,
                         ntree = 300,
                         mtry = 3)

rfmodel2<- randomForest(homeless_individual_rate ~ mgr_studio + fmr_studio, 
                         data = df_pit_count_final,
                         importance = TRUE,
                         ntree = 300,
                         mtry = 3)

rfmodel3<- randomForest(homeless_individual_rate ~ mgr_studio + fmr_studio + unemployment_rate, 
                         data = df_pit_count_final,
                         importance = TRUE,
                         ntree = 300,
                         mtry = 3)

rfmodel4<- randomForest(homeless_individual_rate ~ fmr_studio + min_wage + unemployment_rate + mgr_studio, 
                         data = df_pit_count_final,
                         importance = TRUE,
                         ntree = 300,
                         mtry = 3)

rfmodel5<- randomForest(homeless_individual_rate ~ fmr_studio + min_wage + unemployment_rate + mgr_studio + median_income, 
                         data = df_pit_count_final,
                         importance = TRUE,
                         ntree = 300,
                         mtry = 3)

rfmodel6<- randomForest(homeless_individual_rate ~ fmr_studio + min_wage + unemployment_rate + mgr_studio + median_income + poverty_rate, 
                         data = df_pit_count_final,
                         importance = TRUE,
                         ntree = 300,
                         mtry = 3)


rfmodel1 
rfmodel2
rfmodel3
rfmodel4
rfmodel5
rfmodel6


set.seed(42)
df_pit_split <- initial_split(df_pit_count_final, prop = 0.7)
df_pit_train <- training(df_pit_split)
df_pit_test <- testing(df_pit_split)

# Build random forest model with training dataset with all predictors
set.seed(42)
rfm <- randomForest(homeless_family_rate ~ fmr_studio + min_wage + unemployment_rate + mgr_studio + median_income + poverty_rate, 
                         data = df_pit_train,
                         importance = TRUE,
                         ntree = 300,
                         mtry = 3)
rfm

# reg <- lm(homeless_family_rate ~ fmr_studio + min_wage + unemployment_rate + mgr_studio + median_income + poverty_rate, 
#                          data = df_pit_train)
# summary(reg)
# 
# pred_rfm <- predict(rfm,df_pit_test)
# pred_reg <- predict(reg,df_pit_test)

df_pit_2 <- as.data.frame(cbind(df_pit_test, pred_rfm, pred_reg))
df_pit_2 %>% summarise(homeless_family_rate, round(pred_rfm, 2), pred_reg)

# Run the "rfm" model on the testing dataset and store the values into "hlfr_pred"
hlfr_pred = predict(rfm,df_pit_test)
# Add the hlfr_pred values into the testing dataset
df_pit_test$hlfr_pred = round(hlfr_pred, 2)
#view(df_pit_test)

# Add column "high" where 1 is for homeless_individual_rate >= 0.1 and 0 otherwise. 
# Add column "pred_high" where 1 is for predicted homeless_individual_rate >= 0.1 and 0 otherwise. 
df_pit_test_1 <- df_pit_test %>% 
  mutate(high = if_else(homeless_individual_rate>=0.1, 1, 0)) %>%
  mutate(pred_high = if_else(hlfr_pred>=0.1, 1,0)) 

# Build the table for confision matrix(cfm) for classification of high and low homeless_individual_rate
cfm = table(df_pit_test$high, df_pit_test$pred_high)
cfm
# Calculate the cfm accuracy 
cfm_accuracy = sum(diag(cfm)/sum(cfm))
cfm_accuracy

rfm
plot(rfm)
which.min(rfm$mse)
importance(rfm)
varImpPlot(rfm)

importance_data <- data.frame(Feature = rownames(importance(rfm)),
Importance = importance(rfm)[, "IncNodePurity"])

ggplot(importance_data, aes(x = reorder(Feature, Importance), y = Importance, fill = Feature)) + geom_bar(stat = "identity") +
coord_flip() + labs(title = "Feature Importance", x = "Features", y = "Importance Score") 

rfm$err.rate

plot(rfm, main="OOB Error Rate Across Trees")


 
#https://hackernoon.com/random-forest-regression-in-r-code-and-interpretation
ImpData <- as.data.frame(importance(rfm))
ImpData$Var.Names <- row.names(ImpData)
ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
#https://www.youtube.com/watch?v=1OMJSB7Hxhw


ggplot(df_pit_2, (aes(x = pred_rfm, y = homeless_family_rate - pred_rfm))) +
  geom_point() + 
  labs(x = "Predicted homeless rate family",
       y = "Residual",
       subtitle = "Random forests model") 


# Build scatterplot
ggplot( df_pit_2 ) + 
  geom_point( aes(x = mgr_studio, y = homeless_family_rate, color = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = mgr_studio , y = pred_rfm, color = 'blue',  alpha = 0.5)) + 
  labs(x = "MGR", y = "Rate", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Predicted", "Real"), values = c("blue", "red")) 


library(Metrics)
print(paste0('MAE: ' , mae(df_pit_test$homeless_family_rate,pred_rfm) ))
print(paste0('MSE: ' ,caret::postResample(pred_rfm , df_pit_test$homeless_family_rate)['RMSE']^2 ))
print(paste0('R2: ' ,caret::postResample(pred_rfm , df_pit_test$homeless_family_rate)['Rsquared'] ))


# # If training the model takes too long try setting up lower value of N
# hlf_train <- df_pit_train$homeless_family_rate
# N=50 #length(X_train)
# df_pit_train_ = df_pit_train[1:N , ]
# hlf_train_ = hlf_train[1:N]
# hlf_train_
# 
# seed <-7
# metric<-'RMSE'
# 
# customRF <- list(type = "Regression", library = "randomForest", loop = NULL)
# 
# customRF$parameters <- data.frame(parameter = c("maxnodes", "ntree"), class = rep("numeric", 2), label = c("maxnodes", "ntree"))
# 
# customRF$grid <- function(x, y, len = NULL, search = "grid") {}
# 
# customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
#   randomForest(x, y, maxnodes = param$maxnodes, ntree=param$ntree, ...)
# }
# 
# customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
#    predict(modelFit, newdata)
# customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
#    predict(modelFit, newdata, type = "prob")
# customRF$sort <- function(x) x[order(x[,1]),]
# customRF$levels <- function(x) x$classes
# #install.packages("caret", dependencies = c("Depends", "Suggests"))
# # Set grid search parameters
# control <- trainControl(method="repeatedcv", number=10, repeats=3, search='grid')
# 
# # Outline the grid of parameters
# tunegrid <- expand.grid(.maxnodes=c(10,20,30,50), .ntree=c(300, 500, 600))
# set.seed(seed)
# 
# # Train the model
# rf_gridsearch <- train(x=df_pit_train_, y=hlf_train_, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
# plot(rf_gridsearch)
# rf_gridsearch$bestTune
# varImpPlot(rf_gridsearch$finalModel, main ='Feature importance')
```
